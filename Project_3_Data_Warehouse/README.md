# Sparkify Project: Data Warehouse Modelling with Amazon Redshift
## About the project 

In this project, I applied what i've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. 

To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

---
## Data

There are two different source in S3.

**Song Dataset** : The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
For example:
```
{"num_songs": 1, 
"artist_id": "ARD7TVE1187B99BFB1", 
"artist_latitude": null, 
"artist_longitude": null, 
"artist_location": "California - LA", 
"artist_name": "Casual", 
"song_id": "SOMZWCG12A8C13C480", 
"title": "I Didn't Mean To", 
"duration": 218.93179, 
"year": 0}
```
**Log Dataset** : The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above.
For example:
```
{"artist":"Survivor",
"auth":"Logged In",
"firstName":"Jayden",
"gender":"M",
"itemInSession":0,
"lastName":"Fox",
"length":245.36771,
"level":"free",
"location":"New Orleans-Metairie, LA",
"method":"PUT",
"page":"NextSong",
"registration":1541033612796.0,
"sessionId":100,
"song":"Eye Of The Tiger",
"status":200,
"ts":1541110994796,
"userAgent":"\"Mozilla\/5.0 (Windows NT 6.3; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"",
"userId":"101"}
```

## Schema for Song Play Analysis

Using the song and event datasets, I had need to create a star schema optimized for queries on song play analysis. This includes the following tables.

**Fact Table**
 - songplays : songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

**Dimension Tables** 
  - users : user_id, first_name, last_name, gender, level
  - songs : song_id, title, artist_id, year, duration
  - artists : artist_id, name, location, lattitude, longitude
  - time : start_time, hour, day, week, month, year, weekday

## Project Structure
  `create_table.py` :  is where i created my fact and dimension tables for the star schema in Redshift.
  `etl.py` :   is where i loaded data from S3 into staging tables on Redshift and then processed that data into my analytics tables on Redshift.
  `sql_queries.py` : is where i defined SQL statements, which imported into the two other files above.
  `dwh.cfg` :  is where i kept connection information for Redshift and S3.
  `requirements.txt` :  is where i kept project requirements library above.
  `README.md` :  is where i provided discussion on my process and decisions for this ETL pipeline.


## Requirements
- Python3
- Run `pip install requirements.txt `

## How to Use
- Run `python create_tables.py` command to create database and its tables.
- Run `python etl.py` command to complete etl process.

## The End 
